import torch
import numpy as np

def compute_output_logits(x, vision_model, text_embeddings=None, use_descriptors=False, use_last_n_hidden=None, model_name=None):
    """
    model_fn for CLIP/Other models as required in Cleverhans' attacks.
    :param x: Image tensor.
    :param text_embeddings: Precomputed text embeddings for all labels.
    :param use_similarity: Whether to use cosine similarity (CLIP) or the model's own logits (Resnet, ViT, etc.).
    :return: Similarity scores between the image and all text labels.
    """

    if 'clip' in model_name.lower() or 'llava' in model_name.lower():
        # if not using the last hidden state, use that middle hidden state + layernorm + classifier
        if use_last_n_hidden is None or use_last_n_hidden == 1:
            image_embeds = vision_model(x).image_embeds # (B, 768)
        else:
            image_embeds = vision_model(x).hidden_states[-use_last_n_hidden][:, 0, :]
            image_embeds = list(list(vision_model.named_children())[0][1].named_children())[-1][1](image_embeds) # post layernorm
            image_embeds = vision_model.visual_projection(image_embeds)
        if use_descriptors:
            
            assert type(text_embeddings) == list, "To use descriptors, text_embeddings must be a list of tensors"

            for label in text_embeddings: # label: (B, N_descriptions, 768)
                logit = torch.mm(image_embeds, label.t()).mean(-1).squeeze(0) # average over all descriptions
                logits_per_image.append(logit)

            logits_per_image = torch.stack(logits_per_image)
        else:
            logits_per_image = torch.matmul(image_embeds, text_embeddings.t())
    elif 'vit' in model_name:
        # if not using the last hidden state, use that middle hidden state + layernorm + classifier
        if use_last_n_hidden and use_last_n_hidden > 1:
            logits_per_image = vision_model(x).hidden_states[-use_last_n_hidden][:, 0, :]
            logits_per_image = list(list(vision_model.named_children())[0][1].named_children())[-1][1](logits_per_image) # post layernorm
            logits_per_image = vision_model.classifier(logits_per_image)
        else:
            logits_per_image = vision_model(x).logits

    return logits_per_image


def print_clip_top_probs(logits_per_image, classes):
    """
    Print the top 5 class probabilities for a given image based on its logits.
    :param logits_per_image: Tensor of logits per image generated by the vision model.
    :param classes: List of class names corresponding to each logit in the tensor.
    """

    logit_scale = 2.6592
    logit_scale = torch.tensor(logit_scale).exp()

    logits_per_image *= logit_scale

    probs = logits_per_image.softmax(dim=1)
    top_probs_scaled, top_idxs_scaled = probs[0].topk(5)

    if classes != None:
        for i in range(top_probs_scaled.shape[0]):
            print(f"{classes[top_idxs_scaled[i]]}: {top_probs_scaled[i].item() * 100:.2f}%")   


def boolean_string(s):
    if s not in {'False', 'True'}:
        raise ValueError('Not a valid boolean string')
    return s == 'True'

# Taken from cleverhans library
def zero_out_clipped_grads(grad, x, clip_min, clip_max):
    """
    Helper function to erase entries in the gradient where the update would be
    clipped.
    :param grad: The gradient
    :param x: The current input
    :param clip_min: Minimum input component value
    :param clip_max: Maximum input component value
    """
    signed_grad = torch.sign(grad)

    # Find input components that lie at the boundary of the input range, and
    # where the gradient points in the wrong direction.
    clip_low = torch.le(x, clip_min) & torch.lt(signed_grad, 0)
    clip_high = torch.ge(x, clip_max) & torch.gt(signed_grad, 0)
    clip = clip_low | clip_high
    grad = torch.where(clip, torch.zeros_like(grad), grad)

    return grad


"""Utils for PyTorch"""
# Taken from cleverhans library
def clip_eta(eta, norm, eps):
    """
    PyTorch implementation of the clip_eta in utils_tf.
    :param eta: Tensor
    :param norm: np.inf, 1, or 2
    :param eps: float
    """
    if norm not in [np.inf, 1, 2]:
        raise ValueError("norm must be np.inf, 1, or 2.")

    avoid_zero_div = torch.tensor(1e-12, dtype=eta.dtype, device=eta.device)
    reduc_ind = list(range(1, len(eta.size())))
    if norm == np.inf:
        eta = torch.clamp(eta, -eps, eps)
    else:
        if norm == 1:
            raise NotImplementedError("L1 clip is not implemented.")
            norm = torch.max(
                avoid_zero_div, torch.sum(torch.abs(eta), dim=reduc_ind, keepdim=True)
            )
        elif norm == 2:
            norm = torch.sqrt(
                torch.max(
                    avoid_zero_div, torch.sum(eta ** 2, dim=reduc_ind, keepdim=True)
                )
            )
        factor = torch.min(
            torch.tensor(1.0, dtype=eta.dtype, device=eta.device), eps / norm
        )
        eta *= factor
    return eta



def get_different_class(c_true, classes):
    classes_kept = [c for c in classes if c != c_true]
    new_class_idx = np.random.choice(classes_kept)
    new_label = classes.index(new_class_idx) 
    new_label = torch.tensor(new_label).unsqueeze(0).cuda()
    return new_label